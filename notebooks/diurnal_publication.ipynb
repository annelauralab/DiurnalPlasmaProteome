{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diurnal Rhythm of the Human Plasma Proteome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook and datasets supports the manuscript “Diurnal Rhythm of the Human Plasma Proteome,” which investigates how protein concentrations in human plasma fluctuate over a 24-hour period. Using high-throughput mass spectrometry, we analyzed 208 high-quality plasma samples collected every three hours from 24 healthy individuals under tightly controlled conditions. Out of 523 quantified proteins, 138 (~26%) showed significant diurnal rhythmicity. These proteins were enriched in specific tissues, particularly the liver and platelets, and were involved in key biological pathways including hemostasis, immune signaling, and metabolism. Importantly, 36 clinically relevant biomarkers displayed time-of-day-dependent variation, highlighting the need to incorporate temporal factors in diagnostic and research protocols. This dataset offers valuable insights into circadian regulation of the plasma proteome and provides a resource for researchers studying time-sensitive biomarkers and physiological processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, iplot\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "from pimmslearn.plotting.defaults import color_model_mapping\n",
    "import pimmslearn.plotting.data\n",
    "import pimmslearn.sampling\n",
    "from pimmslearn.sklearn.ae_transformer import AETransformer\n",
    "#from inmoose.pycombat import pycombat_norm\n",
    "from scipy.stats import variation\n",
    "from CosinorPy import cosinor\n",
    "from CosinorPy.cosinor import fit_me\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import fisher_exact\n",
    "\n",
    "\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "init_notebook_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_column_names(df, return_format='name'):\n",
    "    \"\"\"\n",
    "    Clean column names of a DataFrame to extract sample names and filenames.\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame with columns to be cleaned.\n",
    "    return_format (str): Format of the returned DataFrame, either 'name' or 'filename'.\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with cleaned column names.\n",
    "    \"\"\"\n",
    "    cols = pd.DataFrame(df.columns.tolist(), columns = ['filename'])\n",
    "    cols['name']= cols.filename.str.split('\\\\').str[-1].str.split('.raw').str[0]\n",
    "    cols['sample']=cols.name.str.split('_').str[-1].str.split('.raw').str[0]\n",
    "    cols.loc[cols.name.isnull(),'name']=cols.loc[cols.name.isnull()]['filename']\n",
    "    cols.loc[cols['sample'].isnull(),'sample']=cols.loc[cols['sample'].isnull()]['filename']\n",
    "    cols.loc[0:4,return_format]=cols.loc[0:4,'filename']\n",
    "    return cols\n",
    "\n",
    "\n",
    "def filter_missingness(df, feat_prevalence=.2, axis=0):\n",
    "    \"\"\"\n",
    "    Filter features (rows or columns) based on their prevalence in the DataFrame.\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame to filter.\n",
    "    feat_prevalence (float): Minimum fraction of non-missing values required for a feature to be retained.\n",
    "    axis (int): Axis along which to filter (0 for rows, 1 for columns).\n",
    "    Returns:\n",
    "    pd.DataFrame: Filtered DataFrame with features that meet the prevalence threshold.\n",
    "    \"\"\"\n",
    "    N = df.shape[axis]\n",
    "    minimum_freq = N * feat_prevalence\n",
    "    freq = df.notna().sum(axis=axis)\n",
    "    mask = freq >= minimum_freq\n",
    "    print(f\"Drop {(~mask).sum()} along axis {axis}.\")\n",
    "    freq = freq.loc[mask]\n",
    "    if axis == 0:\n",
    "        df = df.loc[:, mask]\n",
    "    else:\n",
    "        df = df.loc[mask]\n",
    "    return df\n",
    "\n",
    "def calculate_cv(df, individual_col):\n",
    "    \"\"\"\n",
    "    Calculate the coefficient of variation (CV) of proteins per individual using scipy.stats.variation.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame where rows are samples, columns are proteins, and one column represents individuals.\n",
    "    individual_col (str): Name of the column indicating individuals.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame with individuals as index and CV values per protein as columns.\n",
    "    \"\"\"\n",
    "    # Exclude the individual column from calculations\n",
    "    protein_cols = df.columns.difference([individual_col])\n",
    "    \n",
    "    # Group by individual and calculate variation for each protein\n",
    "    cv_values = df.groupby(individual_col)[protein_cols].apply(lambda x: variation(x, axis=0, nan_policy='omit'))\n",
    "    \n",
    "    return cv_values\n",
    "\n",
    "\n",
    "def zscore_by_individual(group, protein_cols):\n",
    "    \"\"\"\n",
    "    Z-score each protein across time for a given individual.\n",
    "    Parameters:\n",
    "        group (pd.DataFrame): DataFrame for a single individual with timepoints and protein expression data.\n",
    "        protein_cols (list): List of protein column names.\n",
    "    Returns:\n",
    "        pd.DataFrame: Group with z-scored protein values.\n",
    "    \"\"\"\n",
    "    # Z-score each protein across time for this individual\n",
    "    for protein in protein_cols:\n",
    "        group[protein] = (group[protein] - group[protein].mean()) / group[protein].std()\n",
    "    return group\n",
    "\n",
    "def analyze_circadian_rhythms(df, time_col, protein_cols, period=24, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Perform cosinor analysis on multiple proteins and apply FDR correction.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame with timepoints and protein expression data.\n",
    "        time_col (str): Name of the column with timepoints.\n",
    "        protein_cols (list): List of protein column names.\n",
    "        period (float): Expected circadian period (default: 24 hours).\n",
    "        alpha (float): Significance level for p-value correction (default: 0.05).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Results with amplitude, phase, p-value, and adjusted p-value.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for protein in protein_cols:\n",
    "        time = df[time_col]\n",
    "        expression = df[protein]\n",
    "\n",
    "        # Fit cosinor model\n",
    "        cosinorfx = cosinor(time, expression, period=period)\n",
    "        cosinorfx.fit()\n",
    "\n",
    "        # Collect results\n",
    "        results.append({\n",
    "            \"protein\": protein,\n",
    "            \"amplitude\": cosinorfx.amplitude,\n",
    "            \"phase\": cosinorfx.acrophase,\n",
    "            \"p_value\": cosinorfx.p_value\n",
    "        })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # Apply Benjamini-Hochberg correction for multiple comparisons\n",
    "    results_df[\"adjusted_p_value\"] = multipletests(results_df[\"p_value\"], method=\"fdr_bh\")[1]\n",
    "\n",
    "    # Filter significant circadian proteins\n",
    "    significant_proteins = results_df[results_df[\"adjusted_p_value\"] < alpha]\n",
    "\n",
    "    return significant_proteins\n",
    "\n",
    "def acrophase_hr(phi,A):\n",
    "    \"\"\"\n",
    "    Calculate the acrophase in hours from the phase (phi) and amplitude (A) of a circadian rhythm.\n",
    "    Parameters:\n",
    "    phi (float): Phase of the rhythm in radians.\n",
    "    A (float): Amplitude of the rhythm.\n",
    "    Returns:\n",
    "    float: Acrophase in hours.\n",
    "    \"\"\"\n",
    "    omega = 2 * np.pi / 1440  # 24-hour rhythm in minutes\n",
    "    # Check if the amplitude is negative — flip phase if so\n",
    "    phi_corrected = phi if A >= 0 else phi + np.pi\n",
    "\n",
    "    # Calculate the acrophase time\n",
    "    acrophase_minutes = (-phi_corrected / omega) % 1440\n",
    "    hours = int(acrophase_minutes // 60)\n",
    "    minutes = int(acrophase_minutes % 60)\n",
    "    acrophase_hr = acrophase_minutes / 60\n",
    "    return acrophase_hr\n",
    "\n",
    "def mean_ci(series):\n",
    "    \"\"\"\n",
    "    Calculate the mean, 95% confidence interval, median, and interquartile range (IQR) of a series.\n",
    "    Parameters:\n",
    "    series (pd.Series or np.ndarray): Input data series.\n",
    "    Returns:\n",
    "    pd.Series: A Series containing the mean, lower and upper bounds of the 95% confidence interval, median, and 25th and 75th percentiles (IQR).\n",
    "    \"\"\"\n",
    "    mean = np.mean(series)\n",
    "    n = len(series)\n",
    "    std_err = stats.sem(series)  # Standard error = SD / sqrt(n)\n",
    "    ci = stats.t.ppf(0.975, n-1) * std_err  # 95% CI (two-tailed)\n",
    "    qt25 = np.percentile(series, 25)\n",
    "    qt75 = np.percentile(series, 75)\n",
    "    return pd.Series({'mean': mean, 'CI_lower': mean - ci, 'CI_upper': mean + ci,'median':np.median(series),'q25':qt25,'q75':qt75})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# protein data from DIA-NN:\n",
    "proteome = pd.read_csv('report.pg_matrixCircadian.tsv', sep ='\\t')\n",
    "# sample data from the proteome\n",
    "overview = clean_column_names(proteome)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clan up the overview table:\n",
    "overview['ID']=overview['sample'].str.split('-').str[0]\n",
    "overview['day']=overview['sample'].str.split('-').str[1]\n",
    "overview['timepoint']=overview['sample'].str.split('-').str[2].str.split('.').str[0]\n",
    "overview['plate']= overview.filename.str.split('CircadianRythm_').str[1].str.split('_').str[0]\n",
    "# rename empty or pool samples (Quality control):\n",
    "for col in ['sample','ID']:\n",
    "    overview.loc[overview.filename.str.contains('empty', case = False),col]='Empty'\n",
    "    overview.loc[overview.filename.str.contains('pool', case = False),col]='QCpool'\n",
    "# there are two duplicated samples, rename them:\n",
    "overview.loc[(overview.timepoint.isnull())&(overview.ID!='Empty'),'sample']=overview.loc[(overview.timepoint.isnull())&(overview.ID!='Empty')].filename.str.split('Plate2_').str[1]\n",
    "overview.loc[(overview.timepoint.isnull())&(overview.ID!='Empty'),'ID'] = overview.loc[(overview.timepoint.isnull())&(overview.ID!='Empty')]['sample'].str.split('-').str[0]\n",
    "overview.loc[(overview.timepoint.isnull())&(overview.ID!='Empty'),'day'] = overview.loc[(overview.timepoint.isnull())&(overview.ID!='Empty')]['sample'].str.split('-').str[1]\n",
    "overview.loc[(overview.timepoint.isnull())&(overview.ID!='Empty'),'timepoint'] = overview.loc[(overview.timepoint.isnull())&(overview.ID!='Empty')]['sample'].str.split('-').str[2].str.split('_').str[0]\n",
    "# define the sample type:\n",
    "overview['sample_type']='sample'\n",
    "overview.loc[overview.filename.str.contains('empty', case = False),'sample_type']='Empty'\n",
    "overview.loc[overview.filename.str.contains('pool', case = False),'sample_type']='QCpool'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the proteome data for analysis:\n",
    "index_cols = ['filename','sample','ID','day','timepoint','plate','sample_type']\n",
    "pg_long = proteome.melt(id_vars=proteome.columns[0:4].tolist())\n",
    "pg_long.rename(columns = {'variable':'filename','value':'LFQ'}, inplace = True)\n",
    "pg_long.dropna(subset = ['LFQ'], inplace = True)\n",
    "pg_long =  pd.merge(overview[index_cols], pg_long, on = 'filename')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the number of proteins per sample:\n",
    "count_df = pg_long.fillna('').groupby(index_cols).size().reset_index()\n",
    "count_df.columns = index_cols+['proteins']\n",
    "count_df= count_df.sort_values(['sample_type','proteins'], ascending = [False, False])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 1C: Number of proteins\n",
    "fig1a = px.histogram(count_df[count_df.sample_type=='sample'], x=\"proteins\", color=\"sample_type\",\n",
    "                marginal=\"box\", # or violin, rug\n",
    "                hover_data=count_df.columns,\n",
    "                template = 'simple_white')\n",
    "fig1a.update_layout(bargap = 0.1)\n",
    "fig1a.update_xaxes(range=[200,count_df.proteins.max()+10])\n",
    "fig1a.update_yaxes(matches=None)\n",
    "fig1a.show()  \n",
    "\n",
    "fig1a.write_image('1a.pdf', width = 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter samples on missingness (below 1.5IQR from 25th quantile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = count_df.loc[count_df.sample_type=='sample']\n",
    "# Calculate Q1 and Q3 for the 'proteins' column\n",
    "Q1 = sub_df['proteins'].quantile(0.25)\n",
    "Q3 = sub_df['proteins'].quantile(0.75)\n",
    "\n",
    "# Compute the IQR\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Compute 1.5xIQR\n",
    "threshold = 1.5 * IQR\n",
    "\n",
    "# Calculate the bounds for outliers\n",
    "lower_bound = Q1 - threshold\n",
    "\n",
    "print(f\"Lower Bound: {lower_bound}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclusion_df = count_df.loc[count_df.proteins<=lower_bound]\n",
    "no_exclusion = exclusion_df[exclusion_df.sample_type!='Empty'].shape[0]\n",
    "print(f\"Number of samples below lower bound: {no_exclusion}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude samples before furteher analysis\n",
    "pg_long = pg_long.loc[pg_long['filename'].isin(exclusion_df['filename'])==False]\n",
    "pg_long['filename'].unique().shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter protein values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format the proteome data for further analysis:\n",
    "index_cols = pg_long.columns[0:7]\n",
    "df_wide = pg_long.pivot(index = index_cols, columns='Protein.Group',values = 'LFQ')\n",
    "df_wide = np.log2(df_wide)\n",
    "# filter out proteins with too many missing values, only based on sample_type=='sample (not QC or Empty):\n",
    "pg_wide = df_wide.reset_index()\n",
    "pg_wide_sample = pg_wide.loc[pg_wide.sample_type=='sample'].drop(index_cols[1:], axis = 1).set_index('filename')\n",
    "pg_wide_sample.index.name = 'Sample ID'\n",
    "pg_wide_sample.columns.name = 'protein group'  \n",
    "\n",
    "df = pg_wide.loc[pg_wide.sample_type=='sample'].drop(index_cols[1:], axis = 1).set_index('filename')\n",
    "print(df.shape)\n",
    "\n",
    "SELECT_FEAT=True\n",
    "df = pg_wide_sample.copy()\n",
    "freq_feat = pg_wide_sample.notna().sum()\n",
    "freq_feat.head()  # training data\n",
    "if SELECT_FEAT:\n",
    "    # potentially this can take a few iterations to stabilize.\n",
    "    df = filter_missingness(df, feat_prevalence=.6)\n",
    "    df = filter_missingness(df=df, feat_prevalence=.6, axis=1)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Impute missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overview of the distribution of missing values based on the non-missing intensity values of the protein in question\n",
    "ax = pimmslearn.plotting.data.plot_feat_median_over_prop_missing(\n",
    "    data=pg_wide.drop(index_cols, axis = 1), type='boxplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data format\n",
    "df_wide = pg_wide.reset_index().set_index(['filename'])[df.columns]\n",
    "df_wide.index.name = 'Sample ID'  # already set\n",
    "df_wide.columns.name = 'protein group'  # not set due to csv disk file format\n",
    "print(df_wide.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_X, train_X = pimmslearn.sampling.sample_data(df_wide.stack(),\n",
    "                                           sample_index_to_drop=0,\n",
    "                                           weights=df_wide.notna().sum(),\n",
    "                                           frac=0.1,\n",
    "                                           random_state=42,)\n",
    "val_X, train_X = val_X.unstack(), train_X.unstack()\n",
    "val_X = pd.DataFrame(pd.NA, index=train_X.index,\n",
    "                     columns=train_X.columns).fillna(val_X)\n",
    "\n",
    "model_selected = 'VAE'  # 'DAE'\n",
    "model = AETransformer(\n",
    "    model=model_selected,\n",
    "    hidden_layers=[512,],\n",
    "    latent_dim=50,\n",
    "    out_folder='runs/scikit_interface',\n",
    "    batch_size=10,\n",
    ")\n",
    "\n",
    "model.fit(train_X, val_X,\n",
    "          epochs_max=50,\n",
    "          cuda=False)\n",
    "\n",
    "df_imputed = model.transform(train_X)\n",
    "\n",
    "pred_val = val_X.stack().to_frame('observed')\n",
    "pred_val[model_selected] = df_imputed.stack()\n",
    "val_metrics = pimmslearn.models.calculte_metrics(pred_val, 'observed')\n",
    "\n",
    "pd.DataFrame(val_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imputed = df_imputed.replace(val_X)\n",
    "df_imputed = df_imputed.stack()  # long-format\n",
    "observed = df_imputed.loc[df.index]\n",
    "imputed = df_imputed.loc[df_imputed.index.difference(df.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution of missing value imputations\n",
    "fig, axes = plt.subplots(2, figsize=(8, 4))\n",
    "\n",
    "min_max = pimmslearn.plotting.data.get_min_max_iterable(\n",
    "    [observed, imputed])\n",
    "label_template = '{method} (N={n:,d})'\n",
    "ax, _ = pimmslearn.plotting.data.plot_histogram_intensities(\n",
    "    observed,\n",
    "    ax=axes[0],\n",
    "    min_max=min_max,\n",
    "    label=label_template.format(method='measured',\n",
    "                                n=len(observed),\n",
    "                                ),\n",
    "    color='grey',\n",
    "    alpha=1)\n",
    "_ = ax.legend()\n",
    "ax, _ = pimmslearn.plotting.data.plot_histogram_intensities(\n",
    "    imputed,\n",
    "    ax=axes[1],\n",
    "    min_max=min_max,\n",
    "    label=label_template.format(method=f'{model_selected} imputed',\n",
    "                                n=len(imputed),\n",
    "                                ),\n",
    "    color=color_model_mapping[model_selected],\n",
    "    alpha=1)\n",
    "_ = ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_data = df_imputed.unstack()\n",
    "processed_data = pd.merge(pg_wide[['filename','sample','ID','plate','sample_type']].set_index('filename'), imputed_data, left_index=True, right_index = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_cols = ['filename', 'Timepoints', 'sample', 'ID', 'Plate', 'Sample_type']\n",
    "processed_data.set_index(index_cols, inplace = True)\n",
    "corrected_data = pycombat_norm(processed_data.T,processed_data.reset_index()['Plate'].tolist()).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rythmicity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = corrected_data.copy()\n",
    "df['Time']= df.Timepoints.copy()\n",
    "\n",
    "df = df[['ID','Time']+protein_cols]\n",
    "df.rename(columns = {'ID':'Individual_ID'}, inplace = True)\n",
    "\n",
    "protein_cols = corrected_data.drop(['sample','plate','sample_type','ID','filename','Timepoints'], axis = 1).columns.tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply z-score\n",
    "df_zscored = df.groupby('Individual_ID').apply(zscore_by_individual, protein_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Rythmicity\n",
    "\n",
    "results_z = []\n",
    "\n",
    "for protein_id in protein_cols:  # Loop over each protein column\n",
    "\n",
    "    # Fit the cosinor model\n",
    "    fit = fit_me(df_zscored['Time'], df_zscored[protein_id])\n",
    "\n",
    "    # Extract values from the returned tuple\n",
    "    regression_results, stats, rhythmic_params, part4, part5 = fit\n",
    "\n",
    "    # Store relevant values\n",
    "    results_z.append({\n",
    "        'Protein_ID': protein_id,\n",
    "        'p_value': stats['p'],  # Main p-value for rhythmicity\n",
    "        'p_reject': stats['p_reject'],  # Alternative p-value\n",
    "        'SNR': stats['SNR'],  # Signal-to-Noise Ratio\n",
    "        'RSS': stats['RSS'],  # Residual Sum of Squares\n",
    "        'resid_SE': stats['resid_SE'],  # Residual Standard Error\n",
    "        'ME': stats['ME'],  # Maximum Error\n",
    "        'Amplitude': rhythmic_params['amplitude'],  # Peak-to-trough difference\n",
    "        'Acrophase': rhythmic_params['acrophase'],  # Time of peak\n",
    "        'Mesor': rhythmic_params['mesor'],  # Baseline level\n",
    "        'Period': rhythmic_params['period'],  # Expected period (should be ~24h)\n",
    "        'Max_Location': rhythmic_params['max_loc'],  # Index of highest peak\n",
    "        'Peaks': rhythmic_params['peaks'],  # Array of peak times\n",
    "        'Peak_Heights': rhythmic_params['heights'],  # Heights of peaks\n",
    "        'Troughs': rhythmic_params['troughs'],  # Array of lowest points\n",
    "        'Trough_Heights': rhythmic_params['heights2'],  # Heights of troughs\n",
    "        'Secondary_Period': rhythmic_params['period2']  # Alternative detected period\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results_z to a DataFrame\n",
    "results_z_df = pd.DataFrame(results_z)\n",
    "\n",
    "# Multiple Testing Correction using Benjamini-Hochberg FDR\n",
    "all_p_values = results_z_df['p_value'].values\n",
    "\n",
    "# Apply Benjamini-Hochberg FDR correction\n",
    "corrected_p_values = multipletests(all_p_values, method='fdr_bh')[1]\n",
    "\n",
    "# Add corrected p-values to the DataFrame\n",
    "results_z_df['Corrected_p_values'] = corrected_p_values\n",
    "\n",
    "# Step 5: Filter for significant proteins (optional)\n",
    "significant_proteins_z = results_z_df[results_z_df['Corrected_p_values'] < 0.05]\n",
    "\n",
    "results_z_df[results_z_df['Corrected_p_values'] < 0.05].to_excel('supplementary_table.xlsx', index = None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The number of significantly rythmic proteins is: {}'.format(len(significant_proteins_z)))\n",
    "# save the significant proteinnames:\n",
    "significant_cosinorpy_z = results_z_df[results_z_df.Corrected_p_values<0.05]['Protein_ID'].tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_z_df = pd.merge(proteome[['Protein.Group', 'Genes']], results_z_df, left_on='Protein.Group', right_on='Protein_ID')\n",
    "\n",
    "results_z_df['Uniprot_ID']=results_z_df.Protein_ID.str.split(';').str[0]\n",
    "results_z_df['Gene_ID']=results_z_df.Genes.str.split(';').str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_data = df_zscored[['Time','Individual_ID']+significant_cosinorpy_z].reset_index(drop=True).sort_values(['Time','Individual_ID']).set_index(['Time','Individual_ID']).T\n",
    "heatmap_data_mean =df_zscored[['Time']+significant_cosinorpy_z].reset_index(drop=True).groupby('Time').mean().T\n",
    "heatmap_data_mean = pd.merge(proteome[['Protein.Group','Genes']].drop_duplicates(), heatmap_data_mean, left_on = 'Protein.Group', right_index = True)\n",
    "heatmap_data_mean.Genes = heatmap_data_mean.Genes.str.split(';').str[0]\n",
    "heatmap_data_mean.set_index(['Protein.Group','Genes'], inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "g = sns.clustermap(heatmap_data_mean,\n",
    "                   figsize=(10, 50), #col_colors = group_colors, row_colors = cohort_cluster_colors,\n",
    "                   col_cluster=False, row_cluster=True,\n",
    "                   center = 0,\n",
    "                   linewidth=0.5, linecolor='white', cmap = 'coolwarm')\n",
    "\n",
    "g.figure.savefig(\"fig3_heatmap.svg\")\n",
    "g.figure.savefig(\"fig3_heatmap.pdf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Temporal Dynamics of Diurnal Plasma Protein Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the heatmap order and clusters\n",
    "heatmap_order = heatmap_data_mean.iloc[g.dendrogram_row.reordered_ind].index.get_level_values(\"Protein.Group\").tolist()\n",
    "cluster1 = heatmap_order[0:47]\n",
    "cluster2=heatmap_order[47:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotate cluster to the heatmap data\n",
    "cluster_z = heatmap_data_mean.reset_index().melt(id_vars = ['Protein.Group','Genes'])\n",
    "cluster_z['cluster']='cluster2'\n",
    "cluster_z.loc[cluster_z['Protein.Group'].isin(cluster1),'cluster']='cluster1'\n",
    "# plot the median protein abundance in each cluster with boxplots\n",
    "fig = px.box(cluster_z, x = 'variable', y = 'value', facet_row = 'cluster', height = 500, width = 500,\n",
    "       template='plotly_white')\n",
    "fig.write_image('fig5a.svg')\n",
    "fig.write_image('fig5a.pdf')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the line graphs displaying individual protein trajectories across 24 hoursin each cluster\n",
    "fig = px.line(cluster_z, x='variable',y='value', color = 'Genes', facet_row = 'cluster',\n",
    "              height = 500, width = 550,\n",
    "              template='plotly_white')\n",
    "fig.write_image('fig5b.svg')\n",
    "fig.write_image('fig5b.pdf')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract statistics on each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the names of the proteins in each cluster\n",
    "cluster2 = cluster_z[cluster_z.cluster == 'cluster2']['Protein.Group'].unique().tolist()\n",
    "cluster1 = cluster_z[cluster_z.cluster == 'cluster1']['Protein.Group'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate mean peak time +sd for each cluster:\n",
    "results_z_df['acrophase_hr']=(results_z_df.Acrophase/(2*np.pi))*24+9\n",
    "new_peak = []\n",
    "for _, row in results_z_df[['Acrophase', 'Amplitude']].iterrows():\n",
    "    new_peak_is = acrophase_hr(row['Acrophase'], row['Amplitude'])+9\n",
    "    if new_peak_is>24:\n",
    "        new_peak_is = new_peak_is-24\n",
    "    new_peak.append(new_peak_is) \n",
    "results_z_df['acrophase_hr_new'] = new_peak\n",
    "\n",
    "print(len(cluster1))\n",
    "print(results_z_df[results_z_df['Protein.Group'].isin(cluster1)].acrophase_hr_new.median())\n",
    "print(results_z_df[results_z_df['Protein.Group'].isin(cluster1)].acrophase_hr_new.std())\n",
    "print(len(cluster2))\n",
    "print(results_z_df[results_z_df['Protein.Group'].isin(cluster2)].acrophase_hr_new.median())\n",
    "print(results_z_df[results_z_df['Protein.Group'].isin(cluster2)].acrophase_hr_new.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate confidence intervals\n",
    "sub = results_z_df[results_z_df['Protein.Group'].isin(cluster1+cluster2)].copy()\n",
    "sub['cluster']='cluster1'\n",
    "sub.loc[sub['Protein.Group'].isin(cluster2),'cluster']='cluster2'\n",
    "result = sub.groupby('cluster')['acrophase_hr_new'].apply(mean_ci)\n",
    "\n",
    "# Display result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the polar histogram illustrating the distribution of peak times (acrophases) for diurnal plasma proteins in the two clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase the number of bins for better resolution\n",
    "num_bins = 48  # Adjust this for finer granularity\n",
    "bins = np.linspace(0, 2*np.pi, num_bins + 1)  # Create bin edges\n",
    "\n",
    "results_z_df['acrophase_radians_hr'] = results_z_df['acrophase_hr_new'] * (2 * np.pi / 24)\n",
    "df_phase_only = results_z_df[results_z_df.Protein_ID.isin(significant_cosinorpy_z)].copy()\n",
    "df_phase_only['cluster']='cluster1'\n",
    "df_phase_only.loc[df_phase_only['Protein.Group'].isin(cluster2),'cluster'] = 'cluster2'\n",
    "\n",
    "# Get unique clusters & generate color map\n",
    "unique_clusters = df_phase_only['cluster'].unique()\n",
    "color_map = {cluster: px.colors.qualitative.Plotly[i % len(px.colors.qualitative.Plotly)] \n",
    "             for i, cluster in enumerate(unique_clusters)}\n",
    "\n",
    "# Step 2: Create bins for 48 bins (every 30 minutes, finer granularity)\n",
    "num_bins = 48\n",
    "bins = np.linspace(0, 2 * np.pi, num_bins + 1)  # Create bin edges for radians\n",
    "\n",
    "# Create polar plot\n",
    "fig = go.Figure()\n",
    "\n",
    "# Loop through clusters & plot phase distributions\n",
    "for cluster in unique_clusters:\n",
    "    df_cluster = df_phase_only[df_phase_only['cluster'] == cluster]\n",
    "    \n",
    "    # Compute histogram for this cluster\n",
    "    hist, bin_edges = np.histogram(df_cluster['acrophase_radians_hr'], bins=bins)\n",
    "    \n",
    "    # Compute bin centers in degrees\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2  \n",
    "    bin_centers_deg = np.degrees(bin_centers)  \n",
    "\n",
    "    # Add polar bar trace for this cluster\n",
    "    fig.add_trace(go.Barpolar(\n",
    "        r=hist,  \n",
    "        theta=bin_centers_deg,  \n",
    "        width=360 / num_bins,  \n",
    "        name=f\"Cluster {cluster}\",\n",
    "        marker_color=color_map[cluster],\n",
    "        marker_line_color=\"black\",\n",
    "        marker_line_width=1.5,\n",
    "        opacity=0.8\n",
    "    ))\n",
    "\n",
    "# Format layout\n",
    "fig.update_layout(\n",
    "    title=\"Phase Distribution of Rhythmic Proteins by Cluster\",\n",
    "    polar=dict(\n",
    "        radialaxis=dict(showticklabels=False, ticks=\"\"),  \n",
    "        angularaxis=dict(direction=\"clockwise\",\n",
    "                         tickmode=\"array\",\n",
    "                         tickvals=np.linspace(0, 360, 12),  \n",
    "                         ticktext=[f\"{int(i)}h\" for i in np.linspace(0, 24, 12)])\n",
    "    ),\n",
    "    template=\"plotly_white\"\n",
    ")\n",
    "\n",
    "# Save and show plot\n",
    "fig.write_image('fig5c.svg')\n",
    "fig.write_image('fig5c.pdf')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_z_df[['Protein.Group','Acrophase','acrophase_hr_new']].acrophase_hr_new.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to get meaningful amlitude values, calculate rythmicity for non-zscored data:\n",
    "results = []  # List to store results\n",
    "\n",
    "for protein_id in protein_cols:  # Loop over each protein column\n",
    "\n",
    "    # Fit the cosinor model\n",
    "    fit = fit_me(df['Time'], df[protein_id])\n",
    "\n",
    "    # Extract values from the returned tuple\n",
    "    regression_results, stats, rhythmic_params, part4, part5 = fit\n",
    "\n",
    "    # Store relevant values\n",
    "    results.append({\n",
    "        'Protein_ID': protein_id,\n",
    "        'p_value': stats['p'],  # Main p-value for rhythmicity\n",
    "        'p_reject': stats['p_reject'],  # Alternative p-value\n",
    "        'SNR': stats['SNR'],  # Signal-to-Noise Ratio\n",
    "        'RSS': stats['RSS'],  # Residual Sum of Squares\n",
    "        'resid_SE': stats['resid_SE'],  # Residual Standard Error\n",
    "        'ME': stats['ME'],  # Maximum Error\n",
    "        'Amplitude': rhythmic_params['amplitude'],  # Peak-to-trough difference\n",
    "        'Acrophase': rhythmic_params['acrophase'],  # Time of peak\n",
    "        'Mesor': rhythmic_params['mesor'],  # Baseline level\n",
    "        'Period': rhythmic_params['period'],  # Expected period (should be ~24h)\n",
    "        'Max_Location': rhythmic_params['max_loc'],  # Index of highest peak\n",
    "        'Peaks': rhythmic_params['peaks'],  # Array of peak times\n",
    "        'Peak_Heights': rhythmic_params['heights'],  # Heights of peaks\n",
    "        'Troughs': rhythmic_params['troughs'],  # Array of lowest points\n",
    "        'Trough_Heights': rhythmic_params['heights2'],  # Heights of troughs\n",
    "        'Secondary_Period': rhythmic_params['period2']  # Alternative detected period\n",
    "    })\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the coefficient of variation and the relationship to amplitude and residual standard error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficient of variation\n",
    "df = processed_data.drop(['sample','plate','sample_type'], axis = 1)\n",
    "individual_col='ID'\n",
    "protein_cols = df.columns.difference([individual_col]).tolist()\n",
    "\n",
    "cvs = []\n",
    "for individual in df.ID.unique():\n",
    "    cv_id = []\n",
    "    for protein in protein_cols:\n",
    "        cv_id.append(variation(df[df.ID==individual][protein]))\n",
    "    cvs.append(cv_id)\n",
    "\n",
    "cv = pd.DataFrame(cvs, columns = protein_cols).T\n",
    "cv.columns = df.ID.unique().tolist()\n",
    "cv_df = cv.mean(axis = 1).reset_index()\n",
    "cv_df.columns = ['Protein.Group','CV']\n",
    "cv_df = pd.merge(cv_df, proteome[['Protein.Group','Genes']].drop_duplicates(), on = 'Protein.Group')\n",
    "cv_df['LFQ']=processed_data[protein_cols].mean().tolist()\n",
    "cv_df['completeness']=list(df_wide.notnull().sum()/df_wide.shape[0])\n",
    "cv_df.CV = cv_df.CV*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results combined (to keep track of amplitude and phase)\n",
    "result = pd.merge(results_df[['Protein_ID', 'Amplitude','resid_SE']], results_z_df[['Protein_ID','Gene_ID','Corrected_p_values','acrophase_hr_new']],on = 'Protein_ID')\n",
    "cv_data = result.copy()\n",
    "result = result[['Gene_ID','Protein_ID','Amplitude','acrophase_hr_new','Corrected_p_values']][result.Corrected_p_values<=0.05].copy()\n",
    "result.columns = ['Gene_ID','Protein_ID','Amplitude (from original measurement)','Acrophase (from z-scored data)','Corrected p value']\n",
    "result['Cluster']='cluster1'\n",
    "result.loc[result.Protein_ID.isin(cluster2),'Cluster']='cluster2'\n",
    "result.to_excel('supplementary_table.xlsx', index = None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot CV's\n",
    "cv_data['circadian'] = 'no'\n",
    "cv_data.loc[cv_data.Protein_ID.isin(significant_cosinorpy_z),'circadian']='yes'\n",
    "cv_data['-log10(pval)']=-1*np.log10(cv_data.Corrected_p_values)\n",
    "cv_data = pd.merge(cv_data, cv_df, left_on = 'Protein_ID', right_on = 'Protein.Group')\n",
    "cv_data['Name']=''\n",
    "cv_data.loc[cv_data.circadian=='yes', 'Name'] = cv_data.Gene_ID\n",
    "\n",
    "legend_data = cv_data.iloc[0:2,].copy()\n",
    "legend_data.Amplitude = [0.1, 0.15]\n",
    "legend_data.CV = 12\n",
    "legend_data['-log10(pval)']=[(-np.log10(0.05)),(-np.log10(0.5))]\n",
    "legend_data\n",
    "\n",
    "fig = px.scatter(pd.concat([cv_data.sort_values('circadian', ascending = False), legend_data]), \n",
    "                 y = 'Amplitude', \n",
    "                 x = 'CV', \n",
    "                 #range_x=[0,1],\n",
    "                 color = 'circadian', \n",
    "                 size = '-log10(pval)',\n",
    "                 #text='Name',\n",
    "                 template='none',\n",
    "                 height = 400,\n",
    "                 width=500)\n",
    "\n",
    "fig.update_traces(textposition='middle right', \n",
    "                  textfont=dict(size=10, color='black'),\n",
    "                  marker=dict(line=dict(width=0.5)))\n",
    "\n",
    "fig.write_image('fig2b_amplitude_CV.svg') \n",
    "fig.write_image('fig2b_amplitude_CV.pdf') \n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pearson correlation test\n",
    "corr, p_value = pearsonr(cv_data['CV'], cv_data['Amplitude'])\n",
    "\n",
    "print(f\"Pearson correlation coefficient: {corr:.4f}\")\n",
    "print(f\"P-value: {p_value:.4g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(pd.concat([cv_data.sort_values('circadian', ascending = False), legend_data]), \n",
    "                 y = 'resid_SE', \n",
    "                 x = 'CV', \n",
    "                 #range_x=[0,1],\n",
    "                 color = 'circadian', \n",
    "                 size = '-log10(pval)',\n",
    "                 #text='Name',\n",
    "                 template='none',\n",
    "                 height = 400,\n",
    "                 width=500)\n",
    "\n",
    "fig.update_traces(textposition='middle right', \n",
    "                  textfont=dict(size=10, color='black'),\n",
    "                  marker=dict(line=dict(width=0.5)))\n",
    "\n",
    "fig.write_image('fig2b_resid_CV.svg') \n",
    "fig.write_image('fig2b_resid_CV.pdf') \n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pearson correlation test\n",
    "corr, p_value = pearsonr(cv_data['resid_SE'], cv_data['CV'])\n",
    "\n",
    "print(f\"Pearson correlation coefficient: {corr:.4f}\")\n",
    "print(f\"P-value: {p_value:.4g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate odds ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create a 'CV_group' column based on median\n",
    "median_cv = cv_data['CV'].median()\n",
    "cv_data['CV_group'] = cv_data['CV'].apply(lambda x: 'high' if x > median_cv else 'low')\n",
    "\n",
    "# Step 2: Create a 2x2 contingency table\n",
    "contingency = pd.crosstab(cv_data['circadian'], cv_data['CV_group'])\n",
    "print(\"Contingency table:\\n\", contingency)\n",
    "\n",
    "# Step 3: Perform Fisher's exact test\n",
    "table = [\n",
    "    [contingency.loc['yes', 'high'], contingency.loc['yes', 'low']],\n",
    "    [contingency.loc['no', 'high'], contingency.loc['no', 'low']]\n",
    "]\n",
    "\n",
    "odds_ratio, p_value = fisher_exact(table)\n",
    "\n",
    "print(f\"\\nFisher's Exact Test:\")\n",
    "print(f\"Odds ratio: {odds_ratio:.4f}\")\n",
    "print(f\"P-value: {p_value:.4g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create a 'Amplitude_group' column based on median\n",
    "median_Amplitude = cv_data['Amplitude'].median()\n",
    "cv_data['Amplitude_group'] = cv_data['Amplitude'].apply(lambda x: 'high' if x > median_Amplitude else 'low')\n",
    "\n",
    "# Step 2: Create a 2x2 contingency table\n",
    "contingency = pd.crosstab(cv_data['circadian'], cv_data['Amplitude_group'])\n",
    "print(\"Contingency table:\\n\", contingency)\n",
    "\n",
    "# Step 3: Perform Fisher's exact test\n",
    "table = [\n",
    "    [contingency.loc['yes', 'high'], contingency.loc['yes', 'low']],\n",
    "    [contingency.loc['no', 'high'], contingency.loc['no', 'low']]\n",
    "]\n",
    "\n",
    "odds_ratio, p_value = fisher_exact(table)\n",
    "\n",
    "print(f\"\\nFisher's Exact Test:\")\n",
    "print(f\"Odds ratio: {odds_ratio:.4f}\")\n",
    "print(f\"P-value: {p_value:.4g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enrichment analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tissue and pathway enrichment analyses were conducted for identified protein clusters individually using the Database for Annotation, Visualization, and Integrated Discovery (DAVID) version, 2024 11,12. We provided a list of significant proteins under their official gene symbols (e.g., B2M, ALB) and used Homo sapiens (9606) as the background species. Tissue enrichment was performed with the UP_TISSUE database, excluding pathological, fetal, fluid-based (e.g., serum, plasma, cerebrospinal fluid), and placental enrichments. Pathway enrichment used the REACTOME_PATHWAY database, and we further supported these findings with Gene Ontology Biological Processes (GOTERM_BP_FAT) analysis. Enrichments with Benjamini–Hochberg–adjusted p-values below 0.05 were deemed significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map data to uniprot and GO\n",
    "uniprot_mapping = pd.read_csv('idmapping_2025_03_11.tsv', sep='\\t')\n",
    "# subset to reviewed variants\n",
    "uniprot_mapping = uniprot_mapping[uniprot_mapping['Reviewed']=='reviewed'][['Entry','Entry Name', 'Protein names', 'Gene Names','Gene Ontology (biological process)']]\n",
    "uniprot_mapping = uniprot_mapping[['Entry','Entry Name', 'Protein names', 'Gene Names','Gene Ontology (biological process)']]\n",
    "uniprot_mapping['GO'] = uniprot_mapping['Gene Ontology (biological process)'].str.split('; ')\n",
    "uniprot_mapping = uniprot_mapping.drop('Gene Ontology (biological process)', axis=1).explode('GO').drop_duplicates()\n",
    "# add additional columns to the uniprot mapping for protein.Groups with multiple identifiers\n",
    "additional_columns = []\n",
    "for i in protein_cols:\n",
    "    if ';' in i:\n",
    "        multiple_ids = i.split(';')\n",
    "        combined_df = uniprot_mapping[uniprot_mapping['Entry'].isin(multiple_ids)].astype(str)\n",
    "        collapsed_df = combined_df.apply(lambda col: '; '.join(col)).to_frame().T\n",
    "        collapsed_df.Entry = i\n",
    "        additional_columns.append(collapsed_df)\n",
    "additional_columns = pd.concat(additional_columns)\n",
    "additional_columns['GO'] = additional_columns['GO'].str.split('; ')\n",
    "additional_columns = additional_columns.explode('GO').drop_duplicates()\n",
    "uniprot_mapping = pd.concat([uniprot_mapping, additional_columns]).reset_index(drop=True)\n",
    "uniprot_mapping['Genes']=uniprot_mapping['Gene Names'].str.split(' ').str[0].str.split(';').str[0]\n",
    "# subset to the list of significant proteins\n",
    "mapping = pd.DataFrame(significant_cosinorpy_z, columns = ['Entry'])\n",
    "mapping = pd.merge(mapping, uniprot_mapping[['Entry','Genes']].drop_duplicates(), on = 'Entry', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tissue Enrichemnt Plot\n",
    "tissue = pd.read_excel('Cluster1,2-Tissue.xlsx')\n",
    "tissue['Genes'] = tissue['Genes'].str.split(', ')\n",
    "tissue = tissue.explode('Genes')[['Term','Genes']].drop_duplicates()\n",
    "bar_data = pd.DataFrame(tissue.Term.value_counts()).reset_index()\n",
    "bar_data.rename(columns = {'index':'Term','count':'number of proteins'}, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(bar_data, y = 'Term', x = 'number of proteins', orientation = 'h',\n",
    "       category_orders={'Term':bar_data.sort_values('number of proteins', ascending = False)['Term'].tolist()},\n",
    "       width = 500, height = 500,\n",
    "       template = 'plotly_white')\n",
    "fig.write_image('fig4a.svg')\n",
    "fig.write_image('fig4a.pdf')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot tissue according to heatmap order\n",
    "tissue_genes = tissue['Genes'].unique().tolist()\n",
    "\n",
    "tissue_df = pd.merge(mapping, tissue, on = 'Genes', how = 'outer')\n",
    "tissue_df.fillna('nan', inplace = True)\n",
    "\n",
    "# # add missing Entries\n",
    "tissue_df = pd.merge(pd.DataFrame(heatmap_data_mean.index.get_level_values(\"Protein.Group\").tolist(), columns = ['Entry']), tissue_df[['Entry','Term','Genes']].drop_duplicates(), on = 'Entry', how = 'left')\n",
    "\n",
    "heatmap_order = heatmap_data_mean.iloc[g.dendrogram_row.reordered_ind].index.get_level_values(\"Protein.Group\").tolist()\n",
    "\n",
    "# sort the tissue_df according to heatmap\n",
    "tissue_df['Entry'] = pd.Categorical(tissue_df['Entry'], categories=heatmap_order, ordered=True)\n",
    "tissue_df = tissue_df.sort_values(['Entry'])\n",
    "tissue_df.Entry = tissue_df.Entry.astype(str)\n",
    "\n",
    "# Get unique values of Genes and annotations\n",
    "all_Genes = list(tissue_df['Genes'].unique())\n",
    "all_annotations = list(tissue_df['Term'].unique())\n",
    "\n",
    "# Create indices for sankey source (Geness) and targets (annotations)\n",
    "tissue_df['Genes_index'] = tissue_df['Genes'].apply(lambda x: all_Genes.index(x))\n",
    "tissue_df = tissue_df.sort_values(by='Genes_index').reset_index(drop=True)  # Ensure tissue_df is sorted by the order of 'all_Genes'\n",
    "tissue_df['annotation_index'] = tissue_df['Term'].apply(lambda x: len(all_Genes) + all_annotations.index(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tissue_df.copy()\n",
    "df = df.replace('nan', np.nan)\n",
    "\n",
    "# Count the number of terms per gene\n",
    "df[\"TermCount\"] = df.groupby(\"Genes\")[\"Term\"].transform(\"count\")\n",
    "\n",
    "# Normalize so that all bars have the same height\n",
    "df[\"Proportion\"] = 1 / df[\"TermCount\"]\n",
    "\n",
    "df[\"Term\"] = df[\"Term\"].fillna(\"No Term\")  # Replace NaN with a label\n",
    "\n",
    "# Plot using plotly express\n",
    "fig = px.bar(df, \n",
    "             x=\"Proportion\", \n",
    "             y=\"Genes\", \n",
    "             color=\"Term\", \n",
    "             orientation=\"h\", \n",
    "             barmode=\"stack\", \n",
    "             title=\"Stacked Bar Plot with Equal Heights per Gene\",\n",
    "             height = 2500, width = 300,\n",
    "             category_orders={'Genes':tissue_df[['Genes','Genes_index']].drop_duplicates().sort_values('Genes_index').Genes.tolist()},\n",
    "             template = 'plotly_white')\n",
    "\n",
    "fig.write_image('fig3_tissue_stacked_bar.svg')\n",
    "fig.write_image('fig3_tissue_stacked_bar.pdf')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reactome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read DAVID analysis result\n",
    "pathway = pd.read_excel('Cluster1,2-Reactome.xlsx')\n",
    "pathway['Genes'] = pathway['Genes'].str.split(', ')\n",
    "pathway = pathway.explode('Genes')[['Cluster names','Genes']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the bar chart of the number of proteins in each cluster\n",
    "bar_data = pd.DataFrame(pathway['Cluster names'].value_counts()).reset_index()\n",
    "bar_data.rename(columns = {'index':'Term','count':'number of proteins'}, inplace = True)\n",
    "fig = px.bar(bar_data, y = 'Cluster names', x = 'number of proteins', orientation = 'h',\n",
    "       category_orders={'Cluster names':bar_data.sort_values('number of proteins', ascending = False)['Cluster names'].tolist()},\n",
    "       width = 1000, height = 700,\n",
    "       template = 'plotly_white')  \n",
    "fig.update_layout(showlegend=False)\n",
    "fig.write_image('fig4b.svg')\n",
    "fig.write_image('fig4b.pdf')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sankey for heatmep\n",
    "pathway_genes = pathway['Genes'].unique().tolist()\n",
    "\n",
    "pathway_df = pd.merge(mapping, pathway, on = 'Genes', how = 'outer')\n",
    "pathway_df.fillna('nan', inplace = True)\n",
    "\n",
    "# # add missing Entries\n",
    "pathway_df = pd.merge(pd.DataFrame(heatmap_data_mean.index.get_level_values(\"Protein.Group\").tolist(), columns = ['Entry']), pathway_df[['Entry','Cluster names','Genes']].drop_duplicates(), on = 'Entry', how = 'left')\n",
    "print(pathway_df.shape)\n",
    "\n",
    "sub = pathway_df['Cluster names'].value_counts().reset_index()\n",
    "pathway_df.loc[pathway_df['Cluster names'].isin(sub[sub['count']<=heatmap_data_mean.shape[0]*0.05]['Cluster names']),'Cluster names']='low'\n",
    "\n",
    "\n",
    "heatmap_order = heatmap_data_mean.iloc[g.dendrogram_row.reordered_ind].index.get_level_values(\"Protein.Group\").tolist()\n",
    "\n",
    "# sort the pathway_df according to heatmap\n",
    "pathway_df['Entry'] = pd.Categorical(pathway_df['Entry'], categories=heatmap_order, ordered=True)\n",
    "pathway_df = pathway_df.sort_values(['Entry'])\n",
    "pathway_df.Entry = pathway_df.Entry.astype(str)\n",
    "#pathway_df['Genes'] = pathway_df['Genes'].str.split(' ', expand = True)[0] \n",
    "\n",
    "\n",
    "# Data for Sankey plot\n",
    "import random\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio \n",
    "\n",
    "# Get unique values of Genes and annotations\n",
    "all_Genes = list(pathway_df['Genes'].unique())\n",
    "#all_Genes.reverse()\n",
    "all_annotations = list(pathway_df['Cluster names'].unique())\n",
    "\n",
    "\n",
    "# Create indices for sankey source (Geness) and targets (annotations)\n",
    "pathway_df['Genes_index'] = pathway_df['Genes'].apply(lambda x: all_Genes.index(x))\n",
    "pathway_df = pathway_df.sort_values(by='Genes_index').reset_index(drop=True)  # Ensure pathway_df is sorted by the order of 'all_Genes'\n",
    "pathway_df['annotation_index'] = pathway_df['Cluster names'].apply(lambda x: len(all_Genes) + all_annotations.index(x))\n",
    "\n",
    "# Generate a color for each annotation\n",
    "colors = ['#'+''.join([random.choice('0123456789ABCDEF') for j in range(6)]) for i in range(len(all_annotations))]\n",
    "\n",
    "# Map annotation index to colors\n",
    "target_colors = {len(all_Genes) + i: colors[i] for i in range(len(all_annotations))}\n",
    "\n",
    "# Create a list of colors for the links based on target node\n",
    "link_colors = [target_colors[target] for target in pathway_df['annotation_index']]\n",
    "\n",
    "# Define node positions for source nodes (x = 0.1 for all sources, y spaced evenly)\n",
    "source_y_positions = [i / len(all_Genes) for i in range(len(all_Genes))]  # Evenly spaced source y positions\n",
    "source_x_positions = [0.1] * len(all_Genes)  # Fixed x position for sources\n",
    "\n",
    "# Only set positions for sources, leave target positions to be automatically placed\n",
    "node_x_positions = source_x_positions + [None] * len(all_annotations)  # No x positions for targets\n",
    "node_y_positions = source_y_positions + [None] * len(all_annotations)  # No y positions for targets\n",
    "\n",
    "# Create Sankey diagram\n",
    "fig = go.Figure(go.Sankey(\n",
    "    node = dict(\n",
    "        pad = 15,\n",
    "        thickness = 20,\n",
    "        line = dict(color = \"black\", width = 0.5),\n",
    "        label = all_Genes + all_annotations,\n",
    "        x = node_x_positions,\n",
    "        y = node_y_positions\n",
    "    ),\n",
    "    link = dict(\n",
    "        source = pathway_df['Genes_index'],  # starting points (Geness)\n",
    "        target = pathway_df['annotation_index'],  # end points (annotations)\n",
    "        value = [1] * len(pathway_df),  # each link has equal value (1 for each protein-GO link)\n",
    "        color = link_colors  # set the color of links\n",
    "    )\n",
    "))\n",
    "\n",
    "# Update layout and display the figure\n",
    "fig.update_layout(title_text=\"Sankey Diagram of Protein Genes and Annotations\", \n",
    "                  font_size=10,\n",
    "                  height=800,\n",
    "                  width=600)\n",
    "\n",
    "# Define the path where you want to save the PDF\n",
    "output_path = 'fig3_annotation_heatmap_pathway_annotation.svg'\n",
    "\n",
    "# Save the figure as a PDF\n",
    "pio.write_image(fig, output_path, format='svg')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GO enrichment comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOBP = pd.read_excel('GOBP-Clusters-Refined.xlsx')\n",
    "GOBP['Genes'] = GOBP['Genes'].str.split(', ')\n",
    "GOBP = GOBP.explode('Genes')[['Cluster names','Genes']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bar_data = pd.DataFrame(GOBP['Cluster names'].value_counts()).reset_index()\n",
    "bar_data.rename(columns = {'index':'Term','count':'number of proteins'}, inplace = True)\n",
    "px.bar(bar_data, y = 'Cluster names', x = 'number of proteins', orientation = 'h',category_orders={'Cluster names':bar_data.sort_values('number of proteins', ascending = False)['Cluster names'].tolist()}, template = 'plotly_white') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (PIMMS APS)",
   "language": "python",
   "name": "aps"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
